{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\":\"c451a038-9f30-499b-b652-ec877df6f50d\",\"name\":\"Dr. Jason Mcdowell\",\"email\":\"lewisjennifer@example.com\",\"date\":\"2024-06-04T09:32:45.282497\",\"country\":\"Latvia\",\"company\":\"Cantrell LLC\",\"job\":\"Logistics and distribution manager\",\"phone\":\"(484)200-7378x036\",\"sentence\":\"Become she bad edge maintain.\",\"number\":\"70\",\"timestamp\":\"1.6939533142669601E9\"}'\n",
      "b'{\"id\":\"3e5324e6-0da6-4934-8693-8e5ae46c32d6\",\"name\":\"Douglas Moyer\",\"email\":\"julie58@example.org\",\"date\":\"2024-04-03T23:37:24.041961\",\"country\":\"Romania\",\"company\":\"Smith-Lopez\",\"job\":\"Television camera operator\",\"phone\":\"001-870-384-8740x260\",\"sentence\":\"Mrs other more recently born on.\",\"number\":\"6\",\"timestamp\":\"1.652820360989759E9\"}'\n",
      "b'{\"id\":\"f5b19dcf-cfb8-4583-a2ed-397fc50e7103\",\"name\":\"Michael Valdez\",\"email\":\"bmunoz@example.org\",\"date\":\"2024-01-01T15:12:43.217212\",\"country\":\"Malawi\",\"company\":\"Wagner-Walters\",\"job\":\"Journalist, broadcasting\",\"phone\":\"795.951.1204x19741\",\"sentence\":\"Suffer resource baby trouble country action.\",\"number\":\"68\",\"timestamp\":\"2.845645338474884E8\"}'\n",
      "b'{\"id\":\"0a8c3cc5-a68e-4b4e-bd5c-36cd63e54438\",\"name\":\"Sara Stephenson\",\"email\":\"jamie70@example.com\",\"date\":\"2024-02-20T00:52:19.951591\",\"country\":\"Myanmar\",\"company\":\"Dennis, West and Schmidt\",\"job\":\"Electronics engineer\",\"phone\":\"239.832.9503x9616\",\"sentence\":\"Loss television red stay safe.\",\"number\":\"9\",\"timestamp\":\"7.178751914158001E8\"}'\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "topic_name = 'testout'\n",
    "group_id = 'test'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    group_id=group_id,\n",
    "    auto_offset_reset='earliest', \n",
    "    enable_auto_commit=False )\n",
    "\n",
    "for i,  message in enumerate(consumer):\n",
    "    print(message.value)\n",
    "    if i == 3:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '4043ab72-91cd-47dd-9d1b-8c56a5df8e1b',\n",
       " 'value': {'id': '4043ab72-91cd-47dd-9d1b-8c56a5df8e1b',\n",
       "  'name': 'Charles Kemp',\n",
       "  'email': 'abryant@example.net',\n",
       "  'date': '2024-06-06T04:55:32.828762',\n",
       "  'country': 'Marshall Islands',\n",
       "  'company': 'Hernandez, Guerrero and Stevens',\n",
       "  'job': 'Scientist, audiological',\n",
       "  'phone': '(681)644-4927',\n",
       "  'sentence': 'Yourself fear address recent adult.',\n",
       "  'number': 25}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "a = message.value.decode()\n",
    "\n",
    "json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'STRING',\n",
       " 'name': 'STRING',\n",
       " 'email': 'STRING',\n",
       " 'date': 'STRING',\n",
       " 'country': 'STRING',\n",
       " 'company': 'STRING',\n",
       " 'job': 'STRING',\n",
       " 'phone': 'STRING',\n",
       " 'sentence': 'STRING',\n",
       " 'number': 'INT',\n",
       " 'timestamp': 'DOUBLE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "class DataUtil:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_one_kafka_record(topic_name, bootstrap_servers, group_id=None):\n",
    "        if not group_id:\n",
    "            group_id = 'read_one_record'\n",
    "            \n",
    "        consumer = KafkaConsumer(\n",
    "            topic_name,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            group_id=group_id,\n",
    "            auto_offset_reset='earliest', \n",
    "            enable_auto_commit=False )\n",
    "        try:\n",
    "            for i, c in enumerate(consumer):\n",
    "                if c is not None:\n",
    "                    # this is real string in one record\n",
    "                    return c.value.decode('utf-8')\n",
    "                if i == 10:\n",
    "                    # not sure here needed?\n",
    "                    break\n",
    "            print(\"Not get\")\n",
    "        finally:\n",
    "            consumer.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _infer_kafka_data_schema(input_topic, bootstrap_servers, group_id=None, return_engine='flink'):\n",
    "        # todo: for spark and pyflink schema is different, change it.\n",
    "        kafka_record = DataUtil._get_one_kafka_record(input_topic, bootstrap_servers, group_id=group_id)\n",
    "        if not kafka_record:\n",
    "            print(\"Couldn't get one record from kafka topic: {}\".format(input_topic))\n",
    "            return None\n",
    "\n",
    "        # based on record to get value, and it's schema\n",
    "        record_json = json.loads(kafka_record)\n",
    "        value_json = record_json['value']\n",
    "        \n",
    "        df = pd.json_normalize(value_json)\n",
    "        \n",
    "        if return_engine == 'flink':\n",
    "            schema = {}\n",
    "            for col, dtype in zip(df.columns, df.dtypes):\n",
    "                if dtype == 'int64':\n",
    "                    schema[col] = \"INT\"\n",
    "                elif dtype == 'float64':\n",
    "                    schema[col] = \"DOUBLE\"\n",
    "                elif dtype == 'bool':\n",
    "                    schema[col] = \"BOOLEAN\"\n",
    "                elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                    schema[col] = \"TIMESTAMP\"\n",
    "                else:\n",
    "                    schema[col] = \"STRING\"\n",
    "            return schema\n",
    "        else:\n",
    "            # convert to structure type for spark\n",
    "            schema = {}\n",
    "            for col, dtype in zip(df.columns, df.dtypes):\n",
    "                if dtype == 'int64':\n",
    "                    schema[col] = IntegerType()\n",
    "                elif dtype == 'float64':\n",
    "                    schema[col] = FloatType()\n",
    "                elif dtype == 'bool':\n",
    "                    schema[col] = BooleanType()\n",
    "                else:\n",
    "                    schema[col] = StringType()\n",
    "                    \n",
    "            field_list = []\n",
    "            for c, t in schema.items():\n",
    "                field = StructField(c, t, True)\n",
    "                field_list.append(field)\n",
    "            schema = StructType(field_list) \n",
    "            return schema\n",
    "\n",
    "input_topic = topic_name\n",
    "bootstrap_servers = bootstrap_servers\n",
    "\n",
    "DataUtil._infer_kafka_data_schema(topic_name, bootstrap_servers, return_engine='flink')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
