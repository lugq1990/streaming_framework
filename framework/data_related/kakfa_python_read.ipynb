{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"id\": \"1aa07a59-ce40-4248-af6a-9273d71693f1\", \"name\": \"Isabella Mitchell\", \"email\": \"brianbowman@example.org\", \"date_time\": \"2024-05-22T07:20:12.383027\", \"country\": \"Turkmenistan\", \"company\": \"Brown, Hernandez and King\", \"job\": \"English as a second language teacher\", \"phone\": \"242-707-1499x248\", \"sentence\": \"Thousand million different author say term.\", \"number\": 93}'\n",
      "b'{\"id\": \"036efd9a-ac47-490d-95f0-4c52ab8d9fee\", \"name\": \"Denise Williams\", \"email\": \"christina48@example.com\", \"date_time\": \"2024-02-10T15:11:55.355994\", \"country\": \"Namibia\", \"company\": \"Freeman-Harris\", \"job\": \"Television camera operator\", \"phone\": \"696.429.5574\", \"sentence\": \"Avoid lot travel method race require whole.\", \"number\": 41}'\n",
      "b'{\"id\": \"5192d616-bc41-4726-b0ef-f64e0fe53fd6\", \"name\": \"Andrea White\", \"email\": \"jacksonjason@example.org\", \"date_time\": \"2024-07-01T13:08:46.923739\", \"country\": \"Maldives\", \"company\": \"Wilkins Group\", \"job\": \"Environmental consultant\", \"phone\": \"312.769.8460\", \"sentence\": \"Leg soon consider.\", \"number\": 24}'\n",
      "b'{\"id\": \"bfb4379b-d428-410d-9169-b6bb8ec7dbc1\", \"name\": \"Misty Jones\", \"email\": \"collinskristina@example.com\", \"date_time\": \"2024-06-25T17:47:36.587733\", \"country\": \"Singapore\", \"company\": \"White, Smith and Bradford\", \"job\": \"Clinical scientist, histocompatibility and immunogenetics\", \"phone\": \"670-802-8448\", \"sentence\": \"Budget edge nothing card see.\", \"number\": 98}'\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "topic_name = 'testnew'\n",
    "group_id = 'test'\n",
    "bootstrap_servers = ['localhost:9092']\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=bootstrap_servers,\n",
    "    group_id=group_id,\n",
    "    auto_offset_reset='earliest', \n",
    "    enable_auto_commit=False )\n",
    "\n",
    "for i,  message in enumerate(consumer):\n",
    "    print(message.value)\n",
    "    if i == 3:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': '4043ab72-91cd-47dd-9d1b-8c56a5df8e1b',\n",
       " 'value': {'id': '4043ab72-91cd-47dd-9d1b-8c56a5df8e1b',\n",
       "  'name': 'Charles Kemp',\n",
       "  'email': 'abryant@example.net',\n",
       "  'date': '2024-06-06T04:55:32.828762',\n",
       "  'country': 'Marshall Islands',\n",
       "  'company': 'Hernandez, Guerrero and Stevens',\n",
       "  'job': 'Scientist, audiological',\n",
       "  'phone': '(681)644-4927',\n",
       "  'sentence': 'Yourself fear address recent adult.',\n",
       "  'number': 25}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "a = message.value.decode()\n",
    "\n",
    "json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'STRING',\n",
       " 'name': 'STRING',\n",
       " 'email': 'STRING',\n",
       " 'date': 'STRING',\n",
       " 'country': 'STRING',\n",
       " 'company': 'STRING',\n",
       " 'job': 'STRING',\n",
       " 'phone': 'STRING',\n",
       " 'sentence': 'STRING',\n",
       " 'number': 'INT',\n",
       " 'timestamp': 'DOUBLE'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "class DataUtil:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_one_kafka_record(topic_name, bootstrap_servers, group_id=None):\n",
    "        if not group_id:\n",
    "            group_id = 'read_one_record'\n",
    "            \n",
    "        consumer = KafkaConsumer(\n",
    "            topic_name,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            group_id=group_id,\n",
    "            auto_offset_reset='earliest', \n",
    "            enable_auto_commit=False )\n",
    "        try:\n",
    "            for i, c in enumerate(consumer):\n",
    "                if c is not None:\n",
    "                    # this is real string in one record\n",
    "                    return c.value.decode('utf-8')\n",
    "                if i == 10:\n",
    "                    # not sure here needed?\n",
    "                    break\n",
    "            print(\"Not get\")\n",
    "        finally:\n",
    "            consumer.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _infer_kafka_data_schema(input_topic, bootstrap_servers, group_id=None, return_engine='flink'):\n",
    "        # todo: for spark and pyflink schema is different, change it.\n",
    "        kafka_record = DataUtil._get_one_kafka_record(input_topic, bootstrap_servers, group_id=group_id)\n",
    "        if not kafka_record:\n",
    "            print(\"Couldn't get one record from kafka topic: {}\".format(input_topic))\n",
    "            return None\n",
    "\n",
    "        # based on record to get value, and it's schema\n",
    "        record_json = json.loads(kafka_record)\n",
    "        value_json = record_json['value']\n",
    "        \n",
    "        df = pd.json_normalize(value_json)\n",
    "        \n",
    "        if return_engine == 'flink':\n",
    "            schema = {}\n",
    "            for col, dtype in zip(df.columns, df.dtypes):\n",
    "                if dtype == 'int64':\n",
    "                    schema[col] = \"INT\"\n",
    "                elif dtype == 'float64':\n",
    "                    schema[col] = \"DOUBLE\"\n",
    "                elif dtype == 'bool':\n",
    "                    schema[col] = \"BOOLEAN\"\n",
    "                elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "                    schema[col] = \"TIMESTAMP\"\n",
    "                else:\n",
    "                    schema[col] = \"STRING\"\n",
    "            return schema\n",
    "        else:\n",
    "            # convert to structure type for spark\n",
    "            schema = {}\n",
    "            for col, dtype in zip(df.columns, df.dtypes):\n",
    "                if dtype == 'int64':\n",
    "                    schema[col] = IntegerType()\n",
    "                elif dtype == 'float64':\n",
    "                    schema[col] = FloatType()\n",
    "                elif dtype == 'bool':\n",
    "                    schema[col] = BooleanType()\n",
    "                else:\n",
    "                    schema[col] = StringType()\n",
    "                    \n",
    "            field_list = []\n",
    "            for c, t in schema.items():\n",
    "                field = StructField(c, t, True)\n",
    "                field_list.append(field)\n",
    "            schema = StructType(field_list) \n",
    "            return schema\n",
    "\n",
    "input_topic = topic_name\n",
    "bootstrap_servers = bootstrap_servers\n",
    "\n",
    "DataUtil._infer_kafka_data_schema(topic_name, bootstrap_servers, return_engine='flink')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
